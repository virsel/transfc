{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:05:48.213365Z",
     "start_time": "2024-05-03T13:05:47.776872Z"
    }
   },
   "source": [
    "# read csv with pandas\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '../data_input/train.csv'\n",
    "df = pd.read_csv(data_path, usecols=['id', 'text', 'label'])\n",
    "df.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157789 entries, 0 to 157788\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      157789 non-null  object\n",
      " 1   text    157789 non-null  object\n",
      " 2   label   157789 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:05:51.255588Z",
     "start_time": "2024-05-03T13:05:48.214365Z"
    }
   },
   "source": [
    "import lightning as L\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:05:51.271594Z",
     "start_time": "2024-05-03T13:05:51.256588Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    n_hidden: int\n",
    "    batch_size: int\n",
    "    context_length: int\n",
    "    n_workers: int\n",
    "    # /path/to/save/checkpoints\n",
    "    ckpt_path = None\n",
    "    dropout = 0.1\n",
    "    checkpoint_dir: Path = Path(\"../output/checkpoints/\").absolute()\n",
    "\n",
    "\n",
    "def get_default_config() -> Config:\n",
    "    cfg =  Config(\n",
    "        n_workers=1,\n",
    "        vocab_size=5001,\n",
    "        n_embd=16,\n",
    "        n_hidden=64,\n",
    "        batch_size=32,\n",
    "        context_length=32,\n",
    "    )\n",
    "\n",
    "    # Get the latest checkpoint file path\n",
    "    latest_ckpt_file = sorted(cfg.checkpoint_dir.glob('*.ckpt'), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    cfg.ckpt_path = None if latest_ckpt_file == [] else latest_ckpt_file[0]\n",
    "    return cfg\n",
    "\n",
    "cfg = get_default_config()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:05:51.286763Z",
     "start_time": "2024-05-03T13:05:51.272593Z"
    }
   },
   "source": [
    "def get_trainable_layers(parent_module, parent_name='root'):\n",
    "    res = {}\n",
    "    modules = list(parent_module.named_children())\n",
    "    if len(modules) == 0 and any(param.requires_grad for param in parent_module.parameters()):\n",
    "        # Base case: If no children and has trainable params, return itself\n",
    "        res[parent_name] = parent_module\n",
    "    else:\n",
    "        for name, module in modules:\n",
    "            # Construct the full module name by appending current name to parent's\n",
    "            full_name = f'{parent_name}.{name}' if parent_name else name\n",
    "            # Recursively get trainable layers\n",
    "            sub_layers = get_trainable_layers(module, full_name)\n",
    "            if len(sub_layers) == 0 and any(param.requires_grad for param in module.parameters()):\n",
    "                # If the module has parameters but no sub-layers returned, add the module\n",
    "                res[full_name] = module\n",
    "            res.update(sub_layers)\n",
    "    return res"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:05:51.301762Z",
     "start_time": "2024-05-03T13:05:51.287762Z"
    }
   },
   "source": [
    "def get_activations(parent_module, parent_name='root'):\n",
    "    res = {}\n",
    "    modules = list(parent_module.named_children())\n",
    "    if len(modules) == 0 and isinstance(parent_module, (nn.ReLU, nn.Sigmoid, nn.Tanh)):\n",
    "        # Base case: If no children and has trainable params, return itself\n",
    "        res[parent_name] = parent_module\n",
    "    else:\n",
    "        for name, module in modules:\n",
    "            # Construct the full module name by appending current name to parent's\n",
    "            full_name = f'{parent_name}.{name}' if parent_name else name\n",
    "            # Recursively get trainable layers\n",
    "            sub_layers = get_activations(module, full_name)\n",
    "            if len(sub_layers) == 0 and isinstance(module, (nn.ReLU, nn.Sigmoid, nn.Tanh)):\n",
    "                # If the module has parameters but no sub-layers returned, add the module\n",
    "                res[full_name] = module\n",
    "            res.update(sub_layers)\n",
    "    return res"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:14:10.430942Z",
     "start_time": "2024-05-03T13:14:10.415316Z"
    }
   },
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "class Logger(TensorBoardLogger):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\".\", name=\"lightning_logs\", *args, **kwargs)\n",
    "        self._log_graph = True\n",
    "        self.model = model\n",
    "        self.experiment.add_custom_scalars(self._layout())\n",
    "    \n",
    "    def _layout(self):\n",
    "        activation_params = []\n",
    "        discrepancy_params = []\n",
    "        for i, item in enumerate(self.model.get_activations().items()):\n",
    "            name, layer = item\n",
    "            if isinstance(layer, (nn.ReLU, nn.Sigmoid, nn.Tanh)):\n",
    "                # Adding activation stats layout\n",
    "                # activation_params.append(f'act_{name}_out')\n",
    "                activation_params.append(f'act_{name}_out_sat')\n",
    "    \n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.ndim == 2:\n",
    "                # Assuming you only log update discrepancies for 2D params\n",
    "                formatted_name = 'ud_' + name.replace('.', '_')\n",
    "                discrepancy_params.append(formatted_name)\n",
    "    \n",
    "        layout = {\n",
    "            \"Layer Metrics\": {\n",
    "                \"Activation Out\": [\"Multiline\", activation_params],\n",
    "                \"Update Discrepancy by Layer\": [\"Multiline\", discrepancy_params]\n",
    "            }\n",
    "        }\n",
    "        return layout\n",
    "    \n",
    "    # logging\n",
    "    def log_ud(self):\n",
    "        # Get the current learning rate\n",
    "        lr = self.model.optimizers().param_groups[0]['lr']\n",
    "\n",
    "        # Iterate through named parameters to calculate and log metrics\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.ndim == 2 and p.grad is not None:\n",
    "                # Calculate the standard deviation of the gradients adjusted by the learning rate\n",
    "                grad_std = (lr * p.grad).std()\n",
    "                # Calculate the standard deviation of the parameter values\n",
    "                param_std = p.data.std()\n",
    "                # Calculate the Update Discrepancy (ud) metric and take the log10\n",
    "                metric = (grad_std / param_std).log10().item()\n",
    "                # Create a formatted name that corresponds to the naming convention in the TensorBoard layout\n",
    "                formatted_name = 'ud_' + name.replace('.', '_')\n",
    "                # Log the metric using the formatted name\n",
    "                self.model.log(formatted_name, metric, on_step=False, on_epoch=True)\n",
    "                \n",
    "    def log_activation_out(self):\n",
    "        for i, item in enumerate(self.model.get_activations().items()):\n",
    "            name, layer = item\n",
    "            # if isinstance(layer, (nn.ReLU, nn.Sigmoid, nn.Tanh)):\n",
    "            #     t = layer.out  # Make sure outputs are stored during forward pass\n",
    "            #     self.log(f'act_{name}_out', t.mean().item(), on_step=False, on_epoch=True)\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                t = layer.out  # Make sure outputs are stored during forward pass\n",
    "                saturation = (t < 0.05).float().mean() * 100\n",
    "                self.log(f'act_{name}_out_sat', saturation, on_step=False, on_epoch=True)\n",
    "            if isinstance(layer, nn.Tanh):\n",
    "                t = layer.out  # Make sure outputs are stored during forward pass\n",
    "                saturation = (t.abs() > 0.025).float().mean() * 100\n",
    "                self.log(f'act_{name}_out_sat', saturation, on_step=False, on_epoch=True)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:16:52.000146Z",
     "start_time": "2024-05-03T13:16:51.984519Z"
    }
   },
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(cfg.context_length, cfg.context_length)))\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(cfg.n_embd, cfg.n_embd)\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransfEncModel(L.LightningModule):\n",
    "    def __init__(self, cfg: Config, n_classes=5):\n",
    "        super().__init__()\n",
    "        # Important: This property activates manual optimization.\n",
    "        self.automatic_optimization = False\n",
    "        self.lossi = []\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(cfg.vocab_size, cfg.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(cfg.context_length, cfg.n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(cfg.n_embd, n_head=1) for _ in range(1)])\n",
    "        self.ln_f = nn.LayerNorm(cfg.n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(cfg.context_length * cfg.n_embd, n_classes)\n",
    "        \n",
    "        # register forward hook\n",
    "        self._register_forward_hook()\n",
    "    \n",
    "    def _register_forward_hook(self):\n",
    "        # Define the forward hook function\n",
    "        def forward_hook(module, input, output):\n",
    "            module.out = output  # Store output in the module itself\n",
    "            module.out.retain_grad()  # Ensure that the output gradients are stored\n",
    "                \n",
    "        activations = self.get_activations().values()\n",
    "        for act in activations:\n",
    "            act.register_forward_hook(forward_hook)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T*C)  # (B,T,C) -> (B, T*C)\n",
    "        logits = self.lm_head(x) # (B,n_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        opt = self.optimizers()\n",
    "        opt.zero_grad()\n",
    "        output = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target.view(-1))\n",
    "        # Call backward with retain_graph=True\n",
    "        self.manual_backward(loss, retain_graph=True)\n",
    "        # Ensure that logger has the log_ud method\n",
    "        self.training_step_log()\n",
    "        opt.step()\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def training_step_log(self):\n",
    "        if hasattr(self.logger, 'log_ud'):\n",
    "            self.logger.log_ud()\n",
    "        if hasattr(self.logger, 'log_activation_out'):\n",
    "            self.logger.log_activation_out()\n",
    "    \n",
    "    def on_fit_end(self) -> None:\n",
    "        return super().on_fit_end()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target.view(-1))\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def get_trainable_layers(self):\n",
    "        return get_trainable_layers(self)\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return get_activations(self)"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:16:53.935674Z",
     "start_time": "2024-05-03T13:16:52.017859Z"
    }
   },
   "source": [
    "# split X and Y into train and val, stratify by Y\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from script.data import get_train_test_split\n",
    "\n",
    "Xtr, Xval, Ytr, Yval = get_train_test_split(\n",
    "    df[\"text\"], df[\"label\"], context_length=cfg.context_length, test_size=0.2\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5001\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:16:53.951263Z",
     "start_time": "2024-05-03T13:16:53.935674Z"
    }
   },
   "source": [
    "model = TransfEncModel(cfg)\n",
    "sum([p.nelement() for p in model.parameters()])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86357"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:16:53.966939Z",
     "start_time": "2024-05-03T13:16:53.951263Z"
    }
   },
   "source": [
    "modules = model.get_trainable_layers()\n",
    "modules"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root.token_embedding_table': Embedding(5001, 16),\n",
       " 'root.position_embedding_table': Embedding(32, 16),\n",
       " 'root.blocks.0.sa.heads.0.key': Linear(in_features=16, out_features=16, bias=False),\n",
       " 'root.blocks.0.sa.heads.0.query': Linear(in_features=16, out_features=16, bias=False),\n",
       " 'root.blocks.0.sa.heads.0.value': Linear(in_features=16, out_features=16, bias=False),\n",
       " 'root.blocks.0.sa.proj': Linear(in_features=16, out_features=16, bias=True),\n",
       " 'root.blocks.0.ffwd.net.0': Linear(in_features=16, out_features=64, bias=True),\n",
       " 'root.blocks.0.ffwd.net.2': Linear(in_features=64, out_features=16, bias=True),\n",
       " 'root.blocks.0.ln1': LayerNorm((16,), eps=1e-05, elementwise_affine=True),\n",
       " 'root.blocks.0.ln2': LayerNorm((16,), eps=1e-05, elementwise_affine=True),\n",
       " 'root.ln_f': LayerNorm((16,), eps=1e-05, elementwise_affine=True),\n",
       " 'root.lm_head': Linear(in_features=512, out_features=5, bias=True)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:16:53.976316Z",
     "start_time": "2024-05-03T13:16:53.966939Z"
    }
   },
   "source": [
    "for i, item in enumerate(model.get_activations().items()):\n",
    "    name, layer = item\n",
    "    print(name, layer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root.blocks.0.ffwd.net.1 ReLU()\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:16:53.993317Z",
     "start_time": "2024-05-03T13:16:53.977652Z"
    }
   },
   "source": [
    "# train with pytorch lightning\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(Xtr, Ytr)\n",
    "val_dataset = TensorDataset(Xval, Yval)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, num_workers=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, num_workers=2, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:16:54.008944Z",
     "start_time": "2024-05-03T13:16:53.993317Z"
    }
   },
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "# Example usage with a model instance\n",
    "logger = Logger(model)"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:20:49.156444Z",
     "start_time": "2024-05-03T13:20:06.674287Z"
    }
   },
   "source": [
    "trainer = L.Trainer(max_epochs=2, accelerator='cpu', logger=logger)\n",
    "\n",
    "trainer.fit(model, train_loader)#, val_loader)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\paul-\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name                     | Type       | Params\n",
      "--------------------------------------------------------\n",
      "0 | token_embedding_table    | Embedding  | 80.0 K\n",
      "1 | position_embedding_table | Embedding  | 512   \n",
      "2 | blocks                   | Sequential | 3.2 K \n",
      "3 | ln_f                     | LayerNorm  | 32    \n",
      "4 | lm_head                  | Linear     | 2.6 K \n",
      "--------------------------------------------------------\n",
      "86.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "86.4 K    Total params\n",
      "0.345     Total estimated model params size (MB)\n",
      "C:\\Users\\paul-\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\loggers\\tensorboard.py:194: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "C:\\Users\\paul-\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5b4419f39f14f7b9e2b70c34433d0ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Logger' object has no attribute 'log'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[58], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m L\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m, logger\u001B[38;5;241m=\u001B[39mlogger)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    576\u001B[0m     ckpt_path,\n\u001B[0;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    579\u001B[0m )\n\u001B[1;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:987\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    982\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 987\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    989\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    990\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    992\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1033\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1031\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[0;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m-> 1033\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1034\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[1;32m--> 205\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 363\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:140\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[0;32m    139\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 140\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:252\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    250\u001B[0m             batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautomatic_optimization\u001B[38;5;241m.\u001B[39mrun(trainer\u001B[38;5;241m.\u001B[39moptimizers[\u001B[38;5;241m0\u001B[39m], batch_idx, kwargs)\n\u001B[0;32m    251\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 252\u001B[0m             batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_optimization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[0;32m    256\u001B[0m \u001B[38;5;66;03m# update non-plateau LR schedulers\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\manual.py:94\u001B[0m, in \u001B[0;36m_ManualOptimization.run\u001B[1;34m(self, kwargs)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_run_start()\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m suppress(\u001B[38;5;167;01mStopIteration\u001B[39;00m):  \u001B[38;5;66;03m# no loop to break at this level\u001B[39;00m\n\u001B[1;32m---> 94\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_run_end()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\manual.py:114\u001B[0m, in \u001B[0;36m_ManualOptimization.advance\u001B[1;34m(self, kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m# manually capture logged metrics\u001B[39;00m\n\u001B[1;32m--> 114\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m kwargs  \u001B[38;5;66;03m# release the batch from memory\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()  \u001B[38;5;66;03m# unused hook - call anyway for backward compatibility\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:309\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 309\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m    312\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml_angew_programm\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:391\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[0;32m    390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 391\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[48], line 126\u001B[0m, in \u001B[0;36mTransfEncModel.training_step\u001B[1;34m(self, batch, batch_idx)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_backward(loss, retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    125\u001B[0m \u001B[38;5;66;03m# Ensure that logger has the log_ud method\u001B[39;00m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step_log\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    127\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss, on_step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, prog_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, logger\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, sync_dist\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    130\u001B[0m )\n",
      "Cell \u001B[1;32mIn[48], line 137\u001B[0m, in \u001B[0;36mTransfEncModel.training_step_log\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mlog_ud()\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlog_activation_out\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_activation_out\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[32], line 62\u001B[0m, in \u001B[0;36mLogger.log_activation_out\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     60\u001B[0m     t \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mout  \u001B[38;5;66;03m# Make sure outputs are stored during forward pass\u001B[39;00m\n\u001B[0;32m     61\u001B[0m     saturation \u001B[38;5;241m=\u001B[39m (t \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.05\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mmean() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m---> 62\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog\u001B[49m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mact_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_out_sat\u001B[39m\u001B[38;5;124m'\u001B[39m, saturation, on_step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(layer, nn\u001B[38;5;241m.\u001B[39mTanh):\n\u001B[0;32m     64\u001B[0m     t \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mout  \u001B[38;5;66;03m# Make sure outputs are stored during forward pass\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Logger' object has no attribute 'log'"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(model.get_activations().values()): # note: exclude the output layer\n",
    "  print(layer)\n",
    "  if isinstance(layer, nn.ReLU):\n",
    "    t = layer.out\n",
    "    print('layer %d (%s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "layer2 = None\n",
    "for i, layer in enumerate(model.get_activations().values()): # note: exclude the output layer\n",
    "  if isinstance(layer, nn.ReLU):\n",
    "    layer2 = layer\n",
    "    layer.out.retain_grad()\n",
    "    t = layer.out.grad\n",
    "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "layer2.out.grad",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "layer2.out.retains_grad",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(model.parameters()):\n",
    "  t = p.grad\n",
    "  if p.ndim == 2:\n",
    "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# with torch.no_grad():\n",
    "    # train validation loss\n",
    "# outputs1 = model(Xtr)\n",
    "# loss = F.cross_entropy(outputs1, Ytr)\n",
    "# print(f'Training loss: {loss.item():.4f}')\n",
    "\n",
    "outputs = model(Xval)\n",
    "loss = torch.functional.F.cross_entropy(outputs, Yval)\n",
    "print(f'Validation loss: {loss.item():.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor(1.1228, grad_fn=<NllLossBackward0>)  \n",
    "tensor(1.0828, grad_fn=<NllLossBackward0>)  \n",
    "tensor(1.0767, grad_fn=<NllLossBackward0>) on mlpc  \n",
    "Validation loss: 1.0657 on wavenet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_pred = outputs.argmax(dim=1).detach().numpy()\n",
    "Y_true = Yval.numpy()\n",
    "\n",
    "print(classification_report(Y_true, Y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_angew_programm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
